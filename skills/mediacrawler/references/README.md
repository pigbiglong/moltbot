# MediaCrawler Skill

ä¸€ä¸ªå¼ºå¤§çš„ç¤¾äº¤åª’ä½“æ•°æ®é‡‡é›†å’Œåˆ†æ Skillï¼Œè®© Claude Agent èƒ½å¤Ÿè½»æ¾é‡‡é›†å’Œåˆ†æä¸­å›½ä¸»æµç¤¾äº¤åª’ä½“å¹³å°çš„å…¬å¼€æ•°æ®ã€‚

## ğŸŒŸ ç‰¹æ€§

- **7ä¸ªå¹³å°æ”¯æŒ**: å°çº¢ä¹¦ã€æŠ–éŸ³ã€å¿«æ‰‹ã€Bç«™ã€å¾®åšã€è´´å§ã€çŸ¥ä¹
- **3ç§é‡‡é›†æ¨¡å¼**: å…³é”®è¯æœç´¢ã€æŒ‡å®šå¸–å­è¯¦æƒ…ã€åˆ›ä½œè€…ä¸»é¡µæ•°æ®
- **åŒæ‰§è¡Œæ¨¡å¼**: åŒæ­¥ç­‰å¾…ï¼ˆé€‚åˆå°é‡æ•°æ®ï¼‰å’Œå¼‚æ­¥åå°ï¼ˆé€‚åˆå¤§é‡æ•°æ®ï¼‰
- **æ™ºèƒ½åˆ†æ**: è‡ªåŠ¨ç”Ÿæˆæ•°æ®æ‘˜è¦ã€è¶‹åŠ¿åˆ†æã€æƒ…æ„Ÿåˆ†æ
- **Markdown å¯¼å‡º**: å°† JSON æ•°æ®è½¬æ¢ä¸ºæ˜“è¯»çš„ Markdown æ ¼å¼
- **ç®€å•æ˜“ç”¨**: ä¸€è¡Œä»£ç å¯åŠ¨é‡‡é›†ï¼Œè‡ªåŠ¨å¤„ç†ç™»å½•æ€å’Œæ•°æ®è§£æ

## ğŸ“¦ å®‰è£…

### å‰ç½®ä¾èµ–

ç¡®ä¿å·²å®‰è£…ä»¥ä¸‹ä¾èµ–ï¼š

```bash
pip install httpx aiofiles
```

æˆ–ä½¿ç”¨ uvï¼ˆæ¨èï¼‰ï¼š

```bash
uv pip install httpx aiofiles
```

### æ–‡ä»¶ç»“æ„

```
skills/mediacrawler/
â”œâ”€â”€ skill.md                 # Skill å®šä¹‰ï¼ˆClaude Code å…¥å£ï¼‰
â”œâ”€â”€ __init__.py             # Python åŒ…åˆå§‹åŒ–
â”œâ”€â”€ README.md               # æœ¬æ–‡ä»¶ - å¿«é€Ÿå¼€å§‹æŒ‡å—
â”œâ”€â”€ USAGE.md                # è¯¦ç»†ä½¿ç”¨æŒ‡å—
â”œâ”€â”€ skill_config.py         # å¹³å°é…ç½®
â”œâ”€â”€ skill_wrapper.py        # æ ¸å¿ƒåŒ…è£…å™¨
â”œâ”€â”€ skill_analyzer.py       # æ•°æ®åˆ†æå™¨
â”œâ”€â”€ test_skill.py           # æµ‹è¯•å¥—ä»¶
â””â”€â”€ examples/               # ä½¿ç”¨ç¤ºä¾‹
    â”œâ”€â”€ search_example.py
    â”œâ”€â”€ detail_example.py
    â””â”€â”€ async_example.py
```

## ğŸš€ å¿«é€Ÿå¼€å§‹

### 1. å¯åŠ¨ API æœåŠ¡

```bash
cd /Users/zyjk/Desktop/project/æµ·å¤–æ—…æ¸¸/MediaCrawler
uv run uvicorn api.main:app --port 8080 --reload
```

### 2. å®Œæˆå¹³å°ç™»å½•ï¼ˆé¦–æ¬¡ä½¿ç”¨ï¼‰

```bash
# å°çº¢ä¹¦ç™»å½•ç¤ºä¾‹
uv run python main.py --platform xhs --lt qrcode --type search --keywords "æµ‹è¯•" --max_items 1

# æ‰«ç ç™»å½•åï¼Œç™»å½•æ€ä¼šè‡ªåŠ¨ä¿å­˜
```

### 3. ä½¿ç”¨ Skill

```python
import asyncio
from skills.mediacrawler import MediaCrawlerSkill

async def main():
    async with MediaCrawlerSkill() as skill:
        # æœç´¢å°çº¢ä¹¦ä¸Šå…³äº"AIå·¥å…·"çš„å¸–å­
        result = await skill.crawl(
            platform="xhs",
            mode="search",
            keywords="AIå·¥å…·",
            max_items=10
        )

        print(f"é‡‡é›†äº† {result['summary']['total_posts']} æ¡å¸–å­")
        print(f"å¹³å‡ç‚¹èµ: {result['summary']['avg_likes']}")

asyncio.run(main())
```

## ğŸ“– ä½¿ç”¨ç¤ºä¾‹

### ç¤ºä¾‹ 1: å…³é”®è¯æœç´¢

```python
# æœç´¢å°çº¢ä¹¦ä¸Šçš„æ—…æ¸¸æ”»ç•¥
result = await skill.crawl(
    platform="xhs",
    mode="search",
    keywords="æ—…æ¸¸æ”»ç•¥,æ—…è¡Œæ¨è",
    max_items=20,
    enable_comments=True
)

# æŸ¥çœ‹æ‘˜è¦
summary = result['summary']
print(f"æ€»å¸–å­: {summary['total_posts']}")
print(f"æ€»è¯„è®º: {summary['total_comments']}")
print(f"æœ€çƒ­å¸–å­: {summary['top_post']['title']}")
```

### ç¤ºä¾‹ 2: æŒ‡å®šå¸–å­è¯¦æƒ…

```python
# è·å–ç‰¹å®šå¸–å­çš„è¯¦ç»†ä¿¡æ¯å’Œè¯„è®º
result = await skill.crawl(
    platform="xhs",
    mode="detail",
    post_ids="post_id_1,post_id_2",
    enable_comments=True
)

# åˆ†æè¯„è®ºæƒ…æ„Ÿ
from skills.mediacrawler import analyze_data
import json

with open(result['data_files']['comments'], 'r') as f:
    comments = json.load(f)

sentiment = analyze_data(comments, "xhs", "sentiment")
print(f"æ­£é¢è¯„è®º: {sentiment['positive']['percentage']}%")
print(f"æƒ…æ„Ÿå¾—åˆ†: {sentiment['sentiment_score']}")
```

### ç¤ºä¾‹ 3: å¼‚æ­¥æ¨¡å¼ï¼ˆå¤§é‡æ•°æ®ï¼‰

```python
# å¯åŠ¨å¼‚æ­¥ä»»åŠ¡
task_id = await skill.crawl_async(
    platform="xhs",
    mode="search",
    keywords="Pythonç¼–ç¨‹",
    max_items=100
)

# ç»§ç»­åšå…¶ä»–äº‹æƒ…...
print(f"ä»»åŠ¡ {task_id} æ­£åœ¨åå°è¿è¡Œ")

# ç¨åæ£€æŸ¥çŠ¶æ€
status = await skill.check_status(task_id)
if status['status'] == 'idle':
    result = await skill.get_result(task_id)
```

### ç¤ºä¾‹ 4: æ•°æ®åˆ†æ

```python
from tools.skill_analyzer import analyze_data
import json

# è¯»å–é‡‡é›†çš„æ•°æ®
with open(result['data_files']['contents'], 'r') as f:
    data = json.load(f)

# è¶‹åŠ¿åˆ†æ
trending = analyze_data(data, "xhs", "trending")
print("çƒ­é—¨ä½œè€…:", trending['rising_authors'][:3])
print("å‘å¸ƒé«˜å³°:", trending['peak_times'])

# æ‘˜è¦åˆ†æ
summary = analyze_data(data, "xhs", "summary")
print("äº’åŠ¨æŒ‡æ ‡:", summary['engagement_metrics'])
print("ä½œè€…ç»Ÿè®¡:", summary['author_stats'])
```

## ğŸ¯ æ”¯æŒçš„å¹³å°

| å¹³å°ä»£ç  | å¹³å°åç§° | æ”¯æŒæ¨¡å¼ |
|---------|---------|---------|
| `xhs` | å°çº¢ä¹¦ | search, detail, creator |
| `dy` | æŠ–éŸ³ | search, detail, creator |
| `ks` | å¿«æ‰‹ | search, detail, creator |
| `bili` | Bç«™ | search, detail, creator |
| `wb` | å¾®åš | search, detail, creator |
| `tieba` | è´´å§ | search, detail, creator |
| `zhihu` | çŸ¥ä¹ | search, detail, creator |

### ç¤ºä¾‹ 4: Markdown å¯¼å‡º

```python
from skills.mediacrawler import export_to_markdown

# é‡‡é›†æ•°æ®
result = await skill.crawl(
    platform="xhs",
    mode="search",
    keywords="AIå·¥å…·",
    max_items=10
)

# å¯¼å‡ºä¸º Markdownï¼ˆä¾¿äº Agent é˜…è¯»ï¼‰
md_file = export_to_markdown(
    json_file=result['data_files']['contents'],
    platform="xhs",
    data_type="contents",
    format_type="summary"  # æ‘˜è¦æ ¼å¼ï¼Œæœ€é€‚åˆ Agent
)

print(f"å·²å¯¼å‡º Markdown: {md_file}")

# Agent å¯ä»¥ç›´æ¥ä½¿ç”¨ Read å·¥å…·è¯»å– Markdown
# æ¯” JSON æ›´æ˜“äºç†è§£å’Œåˆ†æ
```

## ğŸ“Š æ•°æ®åˆ†æåŠŸèƒ½

### 1. æ‘˜è¦åˆ†æ (summary)

```python
analysis = analyze_data(data, platform, "summary")
```

è¿”å›ï¼š
- äº’åŠ¨æŒ‡æ ‡ï¼ˆç‚¹èµã€è¯„è®ºã€åˆ†äº«ç»Ÿè®¡ï¼‰
- ä½œè€…ç»Ÿè®¡ï¼ˆæ´»è·ƒä½œè€…ã€å‘å¸–æ•°é‡ï¼‰
- æ—¶é—´åˆ†å¸ƒï¼ˆå‘å¸ƒé«˜å³°æ—¶æ®µï¼‰
- å†…å®¹ç»Ÿè®¡ï¼ˆæ ‡é¢˜é•¿åº¦ç­‰ï¼‰

### 2. è¶‹åŠ¿åˆ†æ (trending)

```python
trending = analyze_data(data, platform, "trending")
```

è¿”å›ï¼š
- çƒ­é—¨å¸–å­ TOP 10
- æ´»è·ƒä½œè€…æ’è¡Œ
- äº’åŠ¨è¶‹åŠ¿ï¼ˆå¢é•¿/ä¸‹é™ï¼‰
- å‘å¸ƒé«˜å³°æ—¶æ®µ

### 3. æƒ…æ„Ÿåˆ†æ (sentiment)

```python
sentiment = analyze_data(comments, platform, "sentiment")
```

è¿”å›ï¼š
- æ­£é¢/è´Ÿé¢/ä¸­æ€§è¯„è®ºæ¯”ä¾‹
- æƒ…æ„Ÿå¾—åˆ† (-1 åˆ° 1)
- è¯„è®ºæ€»æ•°ç»Ÿè®¡

## âš™ï¸ é…ç½®é€‰é¡¹

### MediaCrawlerSkill åˆå§‹åŒ–å‚æ•°

```python
skill = MediaCrawlerSkill(
    api_url="http://localhost:8080",  # API æœåŠ¡åœ°å€
    project_root="/path/to/project",  # é¡¹ç›®æ ¹ç›®å½•ï¼ˆå¯é€‰ï¼‰
    timeout=300                        # è¯·æ±‚è¶…æ—¶æ—¶é—´ï¼ˆç§’ï¼‰
)
```

### crawl æ–¹æ³•å‚æ•°

```python
result = await skill.crawl(
    platform="xhs",           # å¿…éœ€ï¼šå¹³å°ä»£ç 
    mode="search",            # å¿…éœ€ï¼šçˆ¬å–æ¨¡å¼
    keywords="å…³é”®è¯",        # search æ¨¡å¼å¿…éœ€
    post_ids="id1,id2",      # detail æ¨¡å¼å¿…éœ€
    creator_ids="id1,id2",   # creator æ¨¡å¼å¿…éœ€
    max_items=20,            # å¯é€‰ï¼šæœ€å¤§é‡‡é›†æ•°é‡
    enable_comments=True,    # å¯é€‰ï¼šæ˜¯å¦é‡‡é›†è¯„è®º
    timeout=600              # å¯é€‰ï¼šä»»åŠ¡è¶…æ—¶æ—¶é—´
)
```

## ğŸ”§ æ•…éšœæ’é™¤

### é—®é¢˜ 1: APINotAvailableError

**åŸå› **: API æœåŠ¡æœªå¯åŠ¨æˆ–åœ°å€é”™è¯¯

**è§£å†³**:
```bash
# å¯åŠ¨ API æœåŠ¡
uv run uvicorn api.main:app --port 8080

# éªŒè¯æœåŠ¡
curl http://localhost:8080/api/health
```

### é—®é¢˜ 2: LoginRequiredError

**åŸå› **: å¹³å°ç™»å½•æ€ä¸å­˜åœ¨æˆ–å·²è¿‡æœŸ

**è§£å†³**:
```bash
# é‡æ–°ç™»å½•
uv run python main.py --platform xhs --lt qrcode --type search --keywords "æµ‹è¯•" --max_items 1
```

### é—®é¢˜ 3: TaskTimeoutError

**åŸå› **: é‡‡é›†æ•°æ®é‡è¿‡å¤§ï¼Œè¶…è¿‡è¶…æ—¶æ—¶é—´

**è§£å†³**:
- å‡å°‘ `max_items` å‚æ•°
- å¢åŠ  `timeout` å‚æ•°
- ä½¿ç”¨å¼‚æ­¥æ¨¡å¼ `crawl_async()`

### é—®é¢˜ 4: å¯¼å…¥é”™è¯¯

**åŸå› **: ç¼ºå°‘ä¾èµ–åŒ…

**è§£å†³**:
```bash
uv pip install httpx aiofiles
```

## ğŸ“ å®Œæ•´æ–‡æ¡£

æŸ¥çœ‹ `skill.md` è·å–å®Œæ•´çš„ API æ–‡æ¡£å’Œé«˜çº§ç”¨æ³•ã€‚

## ğŸ“ å­¦ä¹ èµ„æº

- **Skill å®šä¹‰**: `skill.md` - Claude Code ä¸»å…¥å£
- **ä½¿ç”¨æŒ‡å—**: `USAGE.md` - å¿«é€Ÿå‚è€ƒå’Œå®ç”¨åœºæ™¯
- **ç¤ºä¾‹ä»£ç **: `examples/` ç›®å½•
- **é¡¹ç›®ä¸»é¡µ**: https://github.com/NanmiCoder/MediaCrawler

## âš ï¸ æ³¨æ„äº‹é¡¹

1. **åˆæ³•åˆè§„**: ä»…ç”¨äºå­¦ä¹ å’Œç ”ç©¶ç›®çš„
2. **é¢‘ç‡æ§åˆ¶**: é¿å…é¢‘ç¹é‡‡é›†ï¼Œéµå®ˆå¹³å°è§„åˆ™
3. **æ•°æ®éšç§**: ä¸è¦é‡‡é›†å’Œä¼ æ’­ç”¨æˆ·éšç§ä¿¡æ¯
4. **èµ„æºå ç”¨**: å¤§é‡é‡‡é›†ä¼šå ç”¨ç³»ç»Ÿèµ„æº
5. **ç™»å½•æ€ç®¡ç†**: ç™»å½•æ€å¯èƒ½è¿‡æœŸï¼Œéœ€å®šæœŸæ›´æ–°

## ğŸ“„ è®¸å¯è¯

æœ¬é¡¹ç›®éµå¾ª NON-COMMERCIAL LEARNING LICENSE 1.1 è®¸å¯è¯ã€‚

## ğŸ¤ è´¡çŒ®

æ¬¢è¿æäº¤ Issue å’Œ Pull Requestï¼

---

**Made with â¤ï¸ for Claude Agent**
