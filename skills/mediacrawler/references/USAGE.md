# MediaCrawler Skill ä½¿ç”¨æŒ‡å—

> ä¸“ä¸º Claude Code è®¾è®¡çš„ç¤¾äº¤åª’ä½“æ•°æ®é‡‡é›† Skill

## ğŸ¯ å¿«é€Ÿå¼€å§‹

### ç¬¬ä¸€æ­¥: å¯åŠ¨ API æœåŠ¡

```bash
cd /Users/zyjk/Desktop/project/æµ·å¤–æ—…æ¸¸/MediaCrawler
uv run uvicorn api.main:app --port 8080
```

### ç¬¬äºŒæ­¥: äº†è§£æ•°æ®ä¿å­˜ä½ç½®

**é‡è¦**ï¼šæ•°æ®å°†è‡ªåŠ¨ä¿å­˜åˆ° Moltbot å·¥ä½œç©ºé—´ï¼š

```bash
# Moltbot å·¥ä½œç©ºé—´
/Users/zyjk/Desktop/project/moltbot/data/
â”œâ”€â”€ xhs/json/       # å°çº¢ä¹¦
â”œâ”€â”€ dy/json/        # æŠ–éŸ³
â”œâ”€â”€ wb/json/        # å¾®åš
â””â”€â”€ ...
```

**æ–‡ä»¶å‘½åè§„åˆ™**ï¼š
- `search_contents_{å…³é”®è¯}_{æ—¶é—´æˆ³}.json` - æœç´¢çš„å¸–å­
- `search_comments_{å…³é”®è¯}_{æ—¶é—´æˆ³}.json` - æœç´¢çš„è¯„è®º
- `detail_contents_{å…³é”®è¯}_{æ—¶é—´æˆ³}.json` - è¯¦æƒ…çš„å¸–å­
- `detail_comments_{å…³é”®è¯}_{æ—¶é—´æˆ³}.json` - è¯¦æƒ…çš„è¯„è®º

### ç¬¬äºŒæ­¥: å®Œæˆå¹³å°ç™»å½•ï¼ˆä»…é¦–æ¬¡ï¼‰

```bash
# å°çº¢ä¹¦ç™»å½•
uv run python main.py --platform xhs --lt qrcode --type search --keywords "æµ‹è¯•" --max_items 1
```

æ‰«æäºŒç»´ç ç™»å½•åï¼Œç™»å½•æ€ä¼šè‡ªåŠ¨ä¿å­˜ã€‚

### ç¬¬ä¸‰æ­¥: å¼€å§‹ä½¿ç”¨

```python
from skills.mediacrawler import MediaCrawlerSkill

async with MediaCrawlerSkill() as skill:
    result = await skill.crawl(
        platform="xhs",
        mode="search",
        keywords="AIå·¥å…·",
        max_items=10
    )
    print(result['summary'])
```

## ğŸ“– æ ¸å¿ƒåŠŸèƒ½

### 1. æ•°æ®é‡‡é›†

#### æœç´¢æ¨¡å¼ - å…³é”®è¯æœç´¢

```python
result = await skill.crawl(
    platform="xhs",           # å¹³å°: xhs/dy/ks/bili/wb/tieba/zhihu
    mode="search",            # æ¨¡å¼: search
    keywords="æ—…æ¸¸æ”»ç•¥,æ—…è¡Œ", # å…³é”®è¯ï¼ˆé€—å·åˆ†éš”ï¼‰
    max_items=20,            # æœ€å¤§é‡‡é›†æ•°é‡
    enable_comments=True     # æ˜¯å¦é‡‡é›†è¯„è®º
)
```

#### è¯¦æƒ…æ¨¡å¼ - æŒ‡å®šå¸–å­

```python
result = await skill.crawl(
    platform="xhs",
    mode="detail",
    post_ids="post_id_1,post_id_2",  # å¸–å­IDï¼ˆé€—å·åˆ†éš”ï¼‰
    enable_comments=True
)
```

#### åˆ›ä½œè€…æ¨¡å¼ - åˆ†æUPä¸»/åšä¸»

```python
result = await skill.crawl(
    platform="bili",
    mode="creator",
    creator_ids="creator_id",  # åˆ›ä½œè€…ID
    max_items=30
)
```

### 2. è¿”å›æ•°æ®ç»“æ„

```python
{
    "status": "success",
    "task_id": "xhs_search_1738051200",
    "summary": {
        "platform": "xhs",
        "platform_name": "å°çº¢ä¹¦",
        "total_posts": 15,        # æ€»å¸–å­æ•°
        "total_comments": 230,    # æ€»è¯„è®ºæ•°
        "avg_likes": 1250.5,      # å¹³å‡ç‚¹èµæ•°
        "max_likes": 5000,        # æœ€é«˜ç‚¹èµæ•°
        "top_post": {             # æœ€çƒ­å¸–å­
            "title": "...",
            "likes": 5000,
            "url": "...",
            "author": "..."
        }
    },
    "data_files": {
        "contents": "data/xhs/json/search_contents_20260128.json",
        "comments": "data/xhs/json/search_comments_20260128.json"
    }
}
```

### 3. æ•°æ®åˆ†æ

```python
from skills.mediacrawler import analyze_data
import json

# è¯»å–æ•°æ®
with open(result['data_files']['contents'], 'r') as f:
    data = json.load(f)

# æ‘˜è¦åˆ†æ
summary = analyze_data(data, platform="xhs", analysis_type="summary")
# è¿”å›: äº’åŠ¨æŒ‡æ ‡ã€ä½œè€…ç»Ÿè®¡ã€æ—¶é—´åˆ†å¸ƒã€å†…å®¹ç»Ÿè®¡

# è¶‹åŠ¿åˆ†æ
trending = analyze_data(data, platform="xhs", analysis_type="trending")
# è¿”å›: çƒ­é—¨å¸–å­TOP10ã€æ´»è·ƒä½œè€…ã€äº’åŠ¨è¶‹åŠ¿ã€å‘å¸ƒé«˜å³°

# æƒ…æ„Ÿåˆ†æï¼ˆè¯„è®ºæ•°æ®ï¼‰
with open(result['data_files']['comments'], 'r') as f:
    comments = json.load(f)
sentiment = analyze_data(comments, platform="xhs", analysis_type="sentiment")
# è¿”å›: æ­£é¢/è´Ÿé¢/ä¸­æ€§æ¯”ä¾‹ã€æƒ…æ„Ÿå¾—åˆ†
```

## ğŸš€ é«˜çº§ç”¨æ³•

### å¼‚æ­¥æ¨¡å¼ï¼ˆå¤§é‡æ•°æ®ï¼‰

```python
# å¯åŠ¨å¼‚æ­¥ä»»åŠ¡
task_id = await skill.crawl_async(
    platform="xhs",
    mode="search",
    keywords="Pythonç¼–ç¨‹",
    max_items=100
)

# ç»§ç»­å…¶ä»–å·¥ä½œ...
print(f"ä»»åŠ¡ {task_id} æ­£åœ¨åå°è¿è¡Œ")

# æ£€æŸ¥çŠ¶æ€
while True:
    status = await skill.check_status(task_id)
    if status["status"] == "idle":
        break
    await asyncio.sleep(3)

# è·å–ç»“æœ
result = await skill.get_result(task_id)
```

### å¤šå¹³å°å¹¶è¡Œé‡‡é›†

```python
# åŒæ—¶é‡‡é›†å¤šä¸ªå¹³å°
xhs_task = await skill.crawl_async(platform="xhs", mode="search", keywords="AIå·¥å…·", max_items=20)
dy_task = await skill.crawl_async(platform="dy", mode="search", keywords="AIå·¥å…·", max_items=20)

# ç­‰å¾…æ‰€æœ‰ä»»åŠ¡å®Œæˆ
tasks = [xhs_task, dy_task]
for task_id in tasks:
    while (await skill.check_status(task_id))["status"] != "idle":
        await asyncio.sleep(3)
    result = await skill.get_result(task_id)
    print(f"{result['summary']['platform_name']}: {result['summary']['total_posts']} æ¡")
```

## ğŸ’¡ å®ç”¨åœºæ™¯

### åœºæ™¯ 1: å¸‚åœºè°ƒç ”

**ç›®æ ‡**: äº†è§£"AIå·¥å…·"å¸‚åœºçš„ç”¨æˆ·éœ€æ±‚å’Œçƒ­ç‚¹

```python
async def market_research():
    async with MediaCrawlerSkill() as skill:
        # é‡‡é›†å°çº¢ä¹¦æ•°æ®
        result = await skill.crawl(
            platform="xhs",
            mode="search",
            keywords="AIå·¥å…·,AIåŠ©æ‰‹,ChatGPT",
            max_items=50,
            enable_comments=True
        )

        # åˆ†æè¶‹åŠ¿
        from skills.mediacrawler import analyze_data
        import json

        with open(result['data_files']['contents'], 'r') as f:
            data = json.load(f)

        trending = analyze_data(data, "xhs", "trending")

        print("=== å¸‚åœºè°ƒç ”æŠ¥å‘Š ===")
        print(f"æ€»å¸–å­æ•°: {result['summary']['total_posts']}")
        print(f"å¹³å‡äº’åŠ¨: {result['summary']['avg_likes']} ç‚¹èµ")
        print(f"\nçƒ­é—¨ä½œè€… TOP 5:")
        for i, author in enumerate(trending['rising_authors'][:5], 1):
            print(f"{i}. {author['author']} - {author['post_count']} ç¯‡å¸–å­")
        print(f"\nå‘å¸ƒé«˜å³°æ—¶æ®µ: {', '.join(trending['peak_times'])}")
```

### åœºæ™¯ 2: ç«å“åˆ†æ

**ç›®æ ‡**: åˆ†æç«å“çš„çˆ†æ¬¾å†…å®¹å’Œç”¨æˆ·åé¦ˆ

```python
async def competitor_analysis(post_ids: str):
    async with MediaCrawlerSkill() as skill:
        # è·å–ç«å“å¸–å­è¯¦æƒ…
        result = await skill.crawl(
            platform="xhs",
            mode="detail",
            post_ids=post_ids,
            enable_comments=True
        )

        # åˆ†æè¯„è®ºæƒ…æ„Ÿ
        from skills.mediacrawler import analyze_data
        import json

        with open(result['data_files']['comments'], 'r') as f:
            comments = json.load(f)

        sentiment = analyze_data(comments, "xhs", "sentiment")

        print("=== ç«å“åˆ†ææŠ¥å‘Š ===")
        print(f"å¸–å­æ•°: {result['summary']['total_posts']}")
        print(f"æ€»è¯„è®º: {result['summary']['total_comments']}")
        print(f"å¹³å‡ç‚¹èµ: {result['summary']['avg_likes']}")
        print(f"\nç”¨æˆ·æƒ…æ„Ÿ:")
        print(f"  æ­£é¢: {sentiment['positive']['percentage']}%")
        print(f"  è´Ÿé¢: {sentiment['negative']['percentage']}%")
        print(f"  æƒ…æ„Ÿå¾—åˆ†: {sentiment['sentiment_score']}")
```

### åœºæ™¯ 3: è¾¾äººç ”ç©¶

**ç›®æ ‡**: åˆ†æå¤´éƒ¨åˆ›ä½œè€…çš„å†…å®¹ç­–ç•¥

```python
async def creator_analysis(creator_id: str):
    async with MediaCrawlerSkill() as skill:
        # é‡‡é›†åˆ›ä½œè€…æ•°æ®
        result = await skill.crawl(
            platform="bili",
            mode="creator",
            creator_ids=creator_id,
            max_items=30
        )

        # åˆ†æå†…å®¹ç­–ç•¥
        from skills.mediacrawler import analyze_data
        import json

        with open(result['data_files']['contents'], 'r') as f:
            data = json.load(f)

        summary = analyze_data(data, "bili", "summary")

        print("=== è¾¾äººåˆ†ææŠ¥å‘Š ===")
        print(f"æ€»è§†é¢‘æ•°: {result['summary']['total_posts']}")
        print(f"å¹³å‡æ’­æ”¾: {result['summary']['avg_likes']}")
        print(f"æœ€çƒ­è§†é¢‘: {result['summary']['top_post']['title']}")
        print(f"\nå†…å®¹ç­–ç•¥:")
        print(f"  å‘å¸ƒé¢‘ç‡: {summary['time_distribution']}")
        print(f"  æœ€ä½³æ—¶æ®µ: {summary.get('peak_hours', 'N/A')}")
```

## âš ï¸ å¸¸è§é—®é¢˜

### Q1: å¦‚ä½•å¤„ç† API è¿æ¥å¤±è´¥ï¼Ÿ

```python
from skills.mediacrawler import MediaCrawlerSkill, APINotAvailableError

try:
    async with MediaCrawlerSkill() as skill:
        result = await skill.crawl(...)
except APINotAvailableError:
    print("âŒ API æœåŠ¡æœªå¯åŠ¨")
    print("è¯·è¿è¡Œ: uv run uvicorn api.main:app --port 8080")
```

### Q2: å¦‚ä½•å¤„ç†ç™»å½•æ€è¿‡æœŸï¼Ÿ

```python
from skills.mediacrawler import LoginRequiredError

try:
    result = await skill.crawl(platform="xhs", ...)
except LoginRequiredError:
    print("âŒ ç™»å½•æ€å·²è¿‡æœŸï¼Œè¯·é‡æ–°ç™»å½•")
    print("è¿è¡Œ: uv run python main.py --platform xhs --lt qrcode --type search --keywords 'æµ‹è¯•' --max_items 1")
```

### Q3: å¦‚ä½•å¤„ç†ä»»åŠ¡è¶…æ—¶ï¼Ÿ

```python
from skills.mediacrawler import TaskTimeoutError

try:
    result = await skill.crawl(
        platform="xhs",
        mode="search",
        keywords="test",
        max_items=10,      # å‡å°‘æ•°é‡
        timeout=600        # å¢åŠ è¶…æ—¶æ—¶é—´
    )
except TaskTimeoutError:
    print("âŒ ä»»åŠ¡è¶…æ—¶ï¼Œå»ºè®®ä½¿ç”¨å¼‚æ­¥æ¨¡å¼")
    task_id = await skill.crawl_async(...)  # æ”¹ç”¨å¼‚æ­¥
```

### Q4: å¦‚ä½•è·å–å¸–å­IDï¼Ÿ

**å°çº¢ä¹¦**:
- URLæ ¼å¼: `https://www.xiaohongshu.com/explore/65a1b2c3d4e5f6g7`
- å¸–å­ID: `65a1b2c3d4e5f6g7`

**æŠ–éŸ³**:
- URLæ ¼å¼: `https://www.douyin.com/video/7123456789012345678`
- è§†é¢‘ID: `7123456789012345678`

**Bç«™**:
- URLæ ¼å¼: `https://www.bilibili.com/video/BV1xx411c7mD`
- è§†é¢‘ID: `BV1xx411c7mD`

## ğŸ“Š æ€§èƒ½å»ºè®®

| æ•°æ®é‡ | æ¨èæ¨¡å¼ | é¢„æœŸè€—æ—¶ | max_items |
|--------|---------|----------|-----------|
| æµ‹è¯• | åŒæ­¥ | 30-60ç§’ | 5-10 |
| å°è§„æ¨¡ | åŒæ­¥ | 1-2åˆ†é’Ÿ | 10-20 |
| ä¸­è§„æ¨¡ | åŒæ­¥/å¼‚æ­¥ | 3-5åˆ†é’Ÿ | 20-50 |
| å¤§è§„æ¨¡ | å¼‚æ­¥ | 5-10åˆ†é’Ÿ | 50-100 |
| å¤šå¹³å° | å¼‚æ­¥å¹¶è¡Œ | è§†ä»»åŠ¡è€Œå®š | - |

## ğŸ”’ æ³¨æ„äº‹é¡¹

1. **åˆæ³•ä½¿ç”¨**: ä»…ç”¨äºå­¦ä¹ å’Œç ”ç©¶ï¼Œéµå®ˆå¹³å°è§„åˆ™
2. **é¢‘ç‡æ§åˆ¶**: é¿å…é¢‘ç¹é‡‡é›†ï¼Œå»ºè®®é—´éš”è‡³å°‘30ç§’
3. **æ•°æ®éšç§**: ä¸è¦é‡‡é›†å’Œä¼ æ’­ç”¨æˆ·éšç§ä¿¡æ¯
4. **èµ„æºç®¡ç†**: å¤§é‡é‡‡é›†ä¼šå ç”¨ç³»ç»Ÿèµ„æºï¼Œæ³¨æ„ç›‘æ§
5. **ç™»å½•ç®¡ç†**: å®šæœŸæ£€æŸ¥ç™»å½•æ€ï¼Œé¿å…é‡‡é›†å¤±è´¥

## ğŸ“š æ›´å¤šèµ„æº

- **å®Œæ•´æ–‡æ¡£**: `skills/mediacrawler/skill.md`
- **ä»£ç ç¤ºä¾‹**: `skills/mediacrawler/examples/`
- **æµ‹è¯•è„šæœ¬**: `skills/mediacrawler/test_skill.py`
- **é¡¹ç›®ä¸»é¡µ**: https://github.com/NanmiCoder/MediaCrawler

---

**ç‰ˆæœ¬**: 1.0.0
**æ›´æ–°æ—¥æœŸ**: 2026-01-28
**é€‚ç”¨äº**: Claude Code
